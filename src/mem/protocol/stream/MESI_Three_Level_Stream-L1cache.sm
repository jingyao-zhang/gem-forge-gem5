/*
 * Copyright (c) 1999-2013 Mark D. Hill and David A. Wood
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are
 * met: redistributions of source code must retain the above copyright
 * notice, this list of conditions and the following disclaimer;
 * redistributions in binary form must reproduce the above copyright
 * notice, this list of conditions and the following disclaimer in the
 * documentation and/or other materials provided with the distribution;
 * neither the name of the copyright holders nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

machine(MachineType:L1Cache, "MESI Directory L1 Cache CMP", interface="AbstractStreamAwareController")
 : CacheMemory * cache;
   RubyPrefetcher * prefetcher;
   bool enable_prefetch := "False";
   int l2_select_num_bits;
   int l2_select_low_bit;
   Cycles l1_request_latency := 2;
   Cycles l1_response_latency := 2;
   Cycles to_l2_latency := 1;
   Cycles l1_stream_message_latency := 2;

   bool multicast_test := "false";

   // Message Buffers between the L1 and the L0 Cache
   // From the L1 cache to the L0 cache
   MessageBuffer * bufferToL0, network="To";

   // From the L0 cache to the L1 cache
   MessageBuffer * bufferFromL0, network="From";

   // From the prefetcher to the L1 cache
   MessageBuffer * prefetchQueue;

   // Message queue from this L1 cache TO the network / L2
   MessageBuffer * requestToL2, network="To", virtual_network="0",
        vnet_type="request";

   MessageBuffer * responseToL2, network="To", virtual_network="1",
        vnet_type="response";
   MessageBuffer * unblockToL2, network="To", virtual_network="2",
        vnet_type="unblock";

   // To this L1 cache FROM the network / L2
   MessageBuffer * requestFromL2, network="From", virtual_network="2",
        vnet_type="request";
   MessageBuffer * responseFromL2, network="From", virtual_network="1",
        vnet_type="response";

{
  // STATES
  state_declaration(State, desc="Cache states", default="L1Cache_State_I") {
    // Base states
    I, AccessPermission:Invalid, desc="a L1 cache entry Idle";
    S, AccessPermission:Read_Only, desc="Shared, copy at L0";
    SS, AccessPermission:Read_Only, desc="Shared, no copy at L0";
    E, AccessPermission:Read_Only, desc="Exclusive, copy at L0";
    EE, AccessPermission:Read_Write, desc="Exclusive, no copy at L0";
    M, AccessPermission:Maybe_Stale, desc="Maybe modified (check Dirty bit), copy at L0", format="!b";
    MM, AccessPermission:Read_Write, desc="Maybe modified (check Dirty bit), no copy at L0", format="!b";

    // Transient States
    IS, AccessPermission:Busy, desc="L1 idle, issued GETS, have not seen response yet";
    IU, AccessPermission:Busy, desc="L1 idle, issued GETU, have not seen response yet";
    IM, AccessPermission:Busy, desc="L1 idle, issued GETX, have not seen response yet";
    SM, AccessPermission:Read_Only, desc="L1 idle, issued GETX, have not seen response yet";
    IS_I, AccessPermission:Busy, desc="L1 idle, issued GETS, saw Inv before data because directory doesn't block on GETS hit";
    M_I, AccessPermission:Busy, desc="L1 replacing, waiting for ACK";
    SINK_WB_ACK, AccessPermission:Busy, desc="This is to sink WB_Acks from L2";

    // For all of the following states, invalidate
    // message has been sent to L0 cache. The response
    // from the L0 cache has not been seen yet.
    S_IL0, AccessPermission:Busy;
    E_IL0, AccessPermission:Busy;
    M_IL0, AccessPermission:Busy;
    MM_IL0, AccessPermission:Read_Write;
    SM_IL0, AccessPermission:Busy;

    // Transient states in which block is being prefetched.
    PF_IS, AccessPermission:Busy, desc="Issued GETS, have not seen response yet";
    PF_IS_I, AccessPermission:Busy, desc="Issued GETS, saw INV before data";
  }

  // EVENTS
  enumeration(Event, desc="Cache events") {
    // Requests from the L0 cache
    Load,                  desc="Load request";
    LoadV,                 desc="Volatile load request, not cache at upper level";
    HitStreamLoadV,        desc="A StreamLoadV that has been hit here";
    LoadU,                 desc="Uncached load request, not cache at this level";
    OffloadStreamLoad,     desc="Receive an offloaded stream request";
    Store,                 desc="Store request";
    WriteBack,             desc="Writeback request";


    // Responses from the L0 Cache
    // L0 cache received the invalidation message
    // and has sent the data.
    L0_DataAck;

    Inv,           desc="Invalidate request from L2 bank";

    // internal generated request
    // Invalidate the line in L0 due to own requirements
    L0_Invalidate_Own;
    // Invalidate the line in L0 due to own prefetch requirements
    PF_L0_Invalidate_Own;
    // Invalidate the line in L0 due to some other cache's requirements
    L0_Invalidate_Else;
    // Invalidate the line in the cache due to some one else / space needs.
    L1_Replacement;
    // Invalidate the line in the cache due to prefetch.
    PF_L1_Replacement;

    // other requests
    Fwd_GETX,   desc="GETX from other processor";
    Fwd_GETS,   desc="GETS from other processor";

    Data,       desc="Data for processor";
    Data_Exclusive,       desc="Data for processor";
    Data_ExclusiveV,      desc="Data for processor no cache";
    DataS_fromL1,         desc="data for GETS request, need to unblock directory";
    Data_all_Acks,        desc="Data for processor, all acks";

    L0_Ack,        desc="Ack for processor";
    Ack,        desc="Ack for processor";
    Ack_all,      desc="Last ack for processor";

    WB_Ack,        desc="Ack for replacement";

    // ! Sean: Add prefetcher at L2.
    PF_Load,         desc="load request from prefetcher.";

    // ! Sean: StreamAwareCache
    L0_StreamConfig, desc="StreamConfig from L0";
    L0_StreamEnd,    desc="StreamEnd from L0";
    L2_StreamData,   desc="StreamData from L2";
    L1_StreamData,   desc="StreamData from L1, need to unblock L2";
    L0_FwdStreamReq, desc="Stream request served by L0 and forwarded to L1";
    L0_StreamNDC,    desc="StreamNDC from L0";
  }

  // TYPES

  // CacheEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry" ) {
    State CacheState,              desc="cache state";
    DataBlock DataBlk,             desc="data for the block";
    bool Dirty, default="false",   desc="data is dirty";
    bool Reused, default="false",  desc="data is reused";
    bool isPrefetch, default="false", desc="data is prefetched & not accessed";
    RequestStatisticPtr ReqStat, default="nullptr", desc="stats of init req for data";
  }

  // TBE fields
  structure(TBE, desc="...") {
    Addr addr,                          desc="Physical address for this TBE";
    State TBEState,                     desc="Transient state";
    DataBlock DataBlk,                  desc="Buffer for the data block";
    bool Dirty, default="false",        desc="data is dirty";
    int pendingAcks, default="0",       desc="number of pending acks";
    bool cachedUpper, default="true",   desc="whether the upper level want to cache it";
    RequestStatisticPtr ReqStat, default="nullptr", desc="stats of init req for data";
  }

  structure(TBETable, external="yes") {
    TBE lookup(Addr);
    void allocate(Addr);
    void deallocate(Addr);
    bool isPresent(Addr);
  }

  TBETable TBEs, template="<L1Cache_TBE>", constructor="m_number_of_TBEs";
  
  /**
   * ! Sean: StreamAwareCache
   */
  structure(MLCStreamEngine, external="yes", desc="Stream Engine at L1") {
    void receiveStreamConfigure(PacketPtr);
    void receiveStreamEnd(PacketPtr);
    void receiveStreamData(ResponseMsg);
    bool isStreamRequest(DynamicStreamSliceId);
    bool isStreamOffloaded(DynamicStreamSliceId);
    bool isStreamCached(DynamicStreamSliceId);
    bool receiveOffloadStreamRequest(DynamicStreamSliceId);
    void receiveOffloadStreamRequestHit(DynamicStreamSliceId);
    void receiveStreamNDCRequest(PacketPtr);
  }

  MLCStreamEngine se, constructor="this, m_bufferToL0_ptr, m_requestToL2_ptr";

  Tick clockEdge();
  Cycles ticksToCycles(Tick t);
  void set_cache_entry(AbstractCacheEntry a);
  void unset_cache_entry();
  void set_tbe(TBE a);
  void unset_tbe();
  void wakeUpBuffers(Addr a);
  void wakeUpAllBuffers(Addr a);
  void profileMsgDelay(int virtualNetworkType, Cycles c);
  int  getRemainTransitions();
  void guaranteeTransition(int);
  void setHitCacheLevel(RequestStatisticPtr, int);
  void addNoCControlMsgs(RequestStatisticPtr, int);
  void addNoCControlEvictMsgs(RequestStatisticPtr, int);
  void addNoCDataMsgs(RequestStatisticPtr, int);
  void recordDeallocateNoReuseReqStats(RequestStatisticPtr, CacheMemory);
  RequestStatisticPtr getRequestStatistic(PacketPtr);
  bool isRequestStatisticStream(RequestStatisticPtr);
  bool isRequestStatisticValid(RequestStatisticPtr);

  bool isStreamSublineEnabled();
  MessageSizeType getMessageSizeType(int);

  // inclusive cache returns L1 entries only
  Entry getCacheEntry(Addr addr), return_by_pointer="yes" {
    Entry cache_entry := static_cast(Entry, "pointer", cache[addr]);
    return cache_entry;
  }

  State getState(TBE tbe, Entry cache_entry, Addr addr) {
    if(is_valid(tbe)) {
      return tbe.TBEState;
    } else if (is_valid(cache_entry)) {
      return cache_entry.CacheState;
    }
    return State:I;
  }

  void setState(TBE tbe, Entry cache_entry, Addr addr, State state) {
    // MUST CHANGE
    if(is_valid(tbe)) {
      tbe.TBEState := state;
    }

    if (is_valid(cache_entry)) {
      cache_entry.CacheState := state;
    }
  }

  AccessPermission getAccessPermission(Addr addr) {
    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      DPRINTF(RubySlicc, "%s %s\n", tbe.TBEState, L1Cache_State_to_permission(tbe.TBEState));
      return L1Cache_State_to_permission(tbe.TBEState);
    }

    Entry cache_entry := getCacheEntry(addr);
    if(is_valid(cache_entry)) {
      DPRINTF(RubySlicc, "%s\n", L1Cache_State_to_permission(cache_entry.CacheState));
      return L1Cache_State_to_permission(cache_entry.CacheState);
    }

    DPRINTF(RubySlicc, "%s\n", AccessPermission:NotPresent);
    return AccessPermission:NotPresent;
  }

  void functionalRead(Addr addr, Packet *pkt) {
    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      testAndRead(addr, tbe.DataBlk, pkt);
    } else {
      testAndRead(addr, getCacheEntry(addr).DataBlk, pkt);
    }
  }

  int functionalWrite(Addr addr, Packet *pkt) {
    int num_functional_writes := 0;

    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      num_functional_writes := num_functional_writes +
        testAndWrite(addr, tbe.DataBlk, pkt);
      return num_functional_writes;
    }

    num_functional_writes := num_functional_writes +
        testAndWrite(addr, getCacheEntry(addr).DataBlk, pkt);
    return num_functional_writes;
  }

  void setAccessPermission(Entry cache_entry, Addr addr, State state) {
    if (is_valid(cache_entry)) {
      cache_entry.changePermission(L1Cache_State_to_permission(state));
    }
  }

  Event mandatory_request_type_to_event(CoherenceClass type) {
    if (type == CoherenceClass:GETS) {
      return Event:Load;
    } else if (type == CoherenceClass:GETU) {
      return Event:LoadV;
    } else if ((type == CoherenceClass:GETX) ||
               (type == CoherenceClass:UPGRADE)) {
      return Event:Store;
    } else if (type == CoherenceClass:PUTX) {
      return Event:WriteBack;
    } else {
      error("Invalid RequestType");
    }
  }

  int getPendingAcks(TBE tbe) {
    return tbe.pendingAcks;
  }

  bool inL0Cache(State state) {
    if (state == State:S || state == State:E || state == State:M ||
        state == State:S_IL0 || state == State:E_IL0 ||
        state == State:M_IL0 || state == State:SM_IL0) {
        return true;
    }

    return false;
  }

  Event prefetch_request_type_to_event(RubyRequestType type) {
    if (type == RubyRequestType:LD) {
      return Event:PF_Load;
    } else if (type == RubyRequestType:IFETCH) {
      error ("Never Inst Prefetch at L2.");
    } else if ((type == RubyRequestType:ST) ||
               (type == RubyRequestType:ATOMIC)) {
      // So far let's only handle load.
      return Event:PF_Load;
    } else {
      error("Invalid RubyRequestType");
    }
  }

  out_port(requestNetwork_out, RequestMsg, requestToL2);
  out_port(responseNetwork_out, ResponseMsg, responseToL2);
  out_port(unblockNetwork_out, ResponseMsg, unblockToL2);
  out_port(bufferToL0_out, CoherenceMsg, bufferToL0);
  out_port(prefetchQueue_out, RubyRequest, prefetchQueue);

  // Response From the L2 Cache to this L1 cache
  in_port(responseNetwork_in, ResponseMsg, responseFromL2, rank = 3) {
    if (responseNetwork_in.isReady(clockEdge())) {
      peek(responseNetwork_in, ResponseMsg) {
        assert(in_msg.Destination.isElement(machineID));

        Entry cache_entry := getCacheEntry(in_msg.addr);
        TBE tbe := TBEs[in_msg.addr];

        // So far we can't handle stream data.
        if (in_msg.sliceIds.isValid()) {
          se.receiveStreamData(in_msg);
          if (machineIDToMachineType(in_msg.Sender) == MachineType:L1Cache) {
            trigger(Event:L1_StreamData, in_msg.addr, cache_entry, tbe);
          } else {
            trigger(Event:L2_StreamData, in_msg.addr, cache_entry, tbe);
          }
        }

        if(in_msg.Type == CoherenceResponseType:DATA_EXCLUSIVE) {
          if (!is_valid(tbe)) {
            assert(multicast_test);
            trigger(Event:Data_Exclusive, in_msg.addr, cache_entry, tbe);
          }
          if (tbe.cachedUpper) {
            trigger(Event:Data_Exclusive, in_msg.addr, cache_entry, tbe);
          } else {
            // ! Sean: StreamAwareCache.
            // This data will not be cached above.
            trigger(Event:Data_ExclusiveV, in_msg.addr, cache_entry, tbe);
          }
        } else if(in_msg.Type == CoherenceResponseType:DATA) {
          if ((getState(tbe, cache_entry, in_msg.addr) == State:IS ||
               getState(tbe, cache_entry, in_msg.addr) == State:IS_I ||
               getState(tbe, cache_entry, in_msg.addr) == State:PF_IS ||
               getState(tbe, cache_entry, in_msg.addr) == State:PF_IS_I) &&
              machineIDToMachineType(in_msg.Sender) == MachineType:L1Cache) {

              trigger(Event:DataS_fromL1, in_msg.addr, cache_entry, tbe);

          } else if ( (getPendingAcks(tbe) - in_msg.AckCount) == 0 ) {
            trigger(Event:Data_all_Acks, in_msg.addr, cache_entry, tbe);
          } else {
            trigger(Event:Data, in_msg.addr, cache_entry, tbe);
          }
        } else if (in_msg.Type == CoherenceResponseType:ACK) {
          if ( (getPendingAcks(tbe) - in_msg.AckCount) == 0 ) {
            trigger(Event:Ack_all, in_msg.addr, cache_entry, tbe);
          } else {
            trigger(Event:Ack, in_msg.addr, cache_entry, tbe);
          }
        } else if (in_msg.Type == CoherenceResponseType:WB_ACK) {
          trigger(Event:WB_Ack, in_msg.addr, cache_entry, tbe);
        } else {
          error("Invalid L1 response type");
        }
      }
    }
  }

  // Request to this L1 cache from the shared L2
  in_port(requestNetwork_in, RequestMsg, requestFromL2, rank = 2) {
    if(requestNetwork_in.isReady(clockEdge())) {
      peek(requestNetwork_in, RequestMsg) {
        assert(in_msg.Destination.isElement(machineID));
        Entry cache_entry := getCacheEntry(in_msg.addr);
        TBE tbe := TBEs[in_msg.addr];

        if (in_msg.Type == CoherenceRequestType:INV) {
            if (is_valid(cache_entry) && inL0Cache(cache_entry.CacheState)) {
              trigger(Event:L0_Invalidate_Else, in_msg.addr,
                      cache_entry, tbe);
            }  else {
              trigger(Event:Inv, in_msg.addr, cache_entry, tbe);
            }
        } else if (in_msg.Type == CoherenceRequestType:GETX ||
              in_msg.Type == CoherenceRequestType:UPGRADE) {
            if (is_valid(cache_entry) && inL0Cache(cache_entry.CacheState)) {
              trigger(Event:L0_Invalidate_Else, in_msg.addr,
                      cache_entry, tbe);
            } else {
              trigger(Event:Fwd_GETX, in_msg.addr, cache_entry, tbe);
            }
        } else if (in_msg.Type == CoherenceRequestType:GETS ||
                   in_msg.Type == CoherenceRequestType:GETU) {
            // For a forwarded GETU request, we treat it as GETS.
            if (is_valid(cache_entry) && inL0Cache(cache_entry.CacheState)) {
              trigger(Event:L0_Invalidate_Else, in_msg.addr,
                      cache_entry, tbe);
            } else {
              trigger(Event:Fwd_GETS, in_msg.addr, cache_entry, tbe);
            }
        } else {
          error("Invalid forwarded request type");
        }
      }
    }
  }

  // Requests to this L1 cache from the L0 cache.
  in_port(messageBufferFromL0_in, CoherenceMsg, bufferFromL0, rank = 1) {
    if (messageBufferFromL0_in.isReady(clockEdge())) {
      peek(messageBufferFromL0_in, CoherenceMsg) {
        Entry cache_entry := getCacheEntry(in_msg.addr);
        TBE tbe := TBEs[in_msg.addr];

        if(in_msg.Class == CoherenceClass:INV_DATA) {
          trigger(Event:L0_DataAck, in_msg.addr, cache_entry, tbe);
        // ! Sean: StreamAwareCache
        } else if (in_msg.Class == CoherenceClass:STREAM_CONFIG) {
          se.receiveStreamConfigure(in_msg.pkt);
          trigger(Event:L0_StreamConfig, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Class == CoherenceClass:STREAM_END) {
          se.receiveStreamEnd(in_msg.pkt);
          trigger(Event:L0_StreamEnd, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Class == CoherenceClass:STREAM_NDC) {
          se.receiveStreamNDCRequest(in_msg.pkt);
          trigger(Event:L0_StreamNDC, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Class == CoherenceClass:INV_ACK) {
          trigger(Event:L0_Ack, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Class == CoherenceClass:STREAM_L0) {
          /**
           * This stream request is served at L0, but the stream is 
           * offloaded, so we should notify the stream engine.
           */
           assert(in_msg.slice.isValid());
           se.receiveOffloadStreamRequestHit(in_msg.slice);
           trigger(Event:L0_FwdStreamReq, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Class == CoherenceClass:STREAM_TO_MLC) {
          /**
           * This stream request should directly be served by MLC SE.
           * Bypass our cache level.
           */
           assert(in_msg.slice.isValid());
           se.receiveOffloadStreamRequest(in_msg.slice);
           // We reuse this event to release the incoming message.
           trigger(Event:L0_FwdStreamReq, in_msg.addr, cache_entry, tbe);
        } else {

          // Stream-aware logic.
          // We check if it's GETU to filter out Fwd_GETS from other stream.
          if (in_msg.Class == CoherenceClass:GETU && se.isStreamRequest(in_msg.slice)) {
            // Stream packet.
            if (is_valid(cache_entry)) {
              // This is a hit StreamLoadV.
              // Hit. Notify the stream engine and response.
              setHitCacheLevel(in_msg.requestStatistic, 1);
              trigger(Event:HitStreamLoadV, in_msg.addr, cache_entry, tbe);
            }
            // Missed.
            if (se.isStreamOffloaded(in_msg.slice)) {
              setHitCacheLevel(in_msg.requestStatistic, 5);
              se.receiveOffloadStreamRequest(in_msg.slice);
              trigger(Event:OffloadStreamLoad, in_msg.addr, cache_entry, tbe);
            } else {
              if (!se.isStreamCached(in_msg.slice)) {
                // Not cached.
                assert(in_msg.Class == CoherenceClass:GETU);
                trigger(Event:LoadU, in_msg.addr, cache_entry, tbe);
              } else {
                // The stream want to be handled normally.
              }
            }
          }

          /**
           * After this point, either not stream packet,
           * or the stream is handled as normal request.
           */

          if (is_valid(cache_entry)) {
            // Hit.
            setHitCacheLevel(in_msg.requestStatistic, 1);
            trigger(mandatory_request_type_to_event(in_msg.Class),
                    in_msg.addr, cache_entry, tbe);
          } else {

            if (cache.cacheAvail(in_msg.addr)) {
              // L1 does't have the line, but we have space for it
              // in the L1 let's see if the L2 has it
              trigger(mandatory_request_type_to_event(in_msg.Class),
                      in_msg.addr, cache_entry, tbe);
            } else {
              // No room in the L1, so we need to make room in the L1
              Entry victim_entry :=
                getCacheEntry(cache.cacheProbe(in_msg.addr));
              TBE victim_tbe := TBEs[cache.cacheProbe(in_msg.addr)];

              if (is_valid(victim_entry) && inL0Cache(victim_entry.CacheState)) {
                trigger(Event:L0_Invalidate_Own,
                        cache.cacheProbe(in_msg.addr),
                        victim_entry, victim_tbe);
              }  else {
                trigger(Event:L1_Replacement,
                        cache.cacheProbe(in_msg.addr),
                        victim_entry, victim_tbe);
              }
            }
          }
        }
      }
    }
  }

  structure(CanStartBulkPrefetchRet, desc="Ret of tryStartBulkPrefetch.") {
    bool canStart, default="false", desc="The bulk prefetch can start";
    bool blockedByLine, default="false", desc="It is blocked by blockLine";
    Addr blockLine, desc="We have to find space for this line.";
  }

  bool isReadyForBulkPrefetch(Addr lineAddr) {
    // A line is considered ready if it's in cache, or has space for prefetching.
    Entry cache_entry := getCacheEntry(lineAddr);
    return is_valid(cache_entry) || cache.cacheAvail(lineAddr);
  }

  CanStartBulkPrefetchRet canStartBulkPrefetch(RubyRequest in_msg) {
    // Slicc has no loop, I manually unroll it...
    int bulkSize := in_msg.addrBulk.size();
    assert(bulkSize <= 4);
    CanStartBulkPrefetchRet ret;
    int idx := in_msg.addrBulk.getCurrentIdxInBulk();
    if (idx < bulkSize) {
      Addr lineAddr := in_msg.addrBulk.getAt(idx);
      if (!isReadyForBulkPrefetch(lineAddr)) {
        ret.blockedByLine := true;
        ret.blockLine := lineAddr;
        return ret;
      }
    }
    idx := idx + 1;
    if (idx < bulkSize) {
      Addr lineAddr := in_msg.addrBulk.getAt(idx);
      if (!isReadyForBulkPrefetch(lineAddr)) {
        ret.blockedByLine := true;
        ret.blockLine := lineAddr;
        return ret;
      }
    }
    idx := idx + 1;
    if (idx < bulkSize) {
      Addr lineAddr := in_msg.addrBulk.getAt(idx);
      if (!isReadyForBulkPrefetch(lineAddr)) {
        ret.blockedByLine := true;
        ret.blockLine := lineAddr;
        return ret;
      }
    }
    idx := idx + 1;
    if (idx < bulkSize) {
      Addr lineAddr := in_msg.addrBulk.getAt(idx);
      if (!isReadyForBulkPrefetch(lineAddr)) {
        ret.blockedByLine := true;
        ret.blockLine := lineAddr;
        return ret;
      }
    }
    idx := idx + 1;
    // All lines in the bulk are ready.
    int numLinesInBulk := bulkSize - idx;
    if (getRemainTransitions() < numLinesInBulk) {
      // We hack here to make sure we have enough transistions.
      guaranteeTransition(bulkSize);
    }
    assert(getRemainTransitions() >= numLinesInBulk);
    // // Check for TBEs and RequestQueue.
    // if (!TBEs.areNSlotsAvailable(numLinesInBulk, clockEdge())) {
    //   ret.canStart := false;
    //   return ret;
    // }
    // if (!requestToL2.areNSlotsAvailable(numLinesInBulk, clockEdge())) {
    //   ret.canStart := false;
    //   return ret;
    // }
    ret.canStart := true;
    return ret;
  }

  // Prefetch queue between the controller and the prefetcher.
  in_port(prefetchQueue_in, RubyRequest, prefetchQueue, desc="...", rank=0) {
    if (prefetchQueue_in.isReady(clockEdge())) {
      peek(prefetchQueue_in, RubyRequest) {
        /**
         * We add support for bulk prefetch.
         * NOTE: always use in_msg.addrBulk, as we can't get a reference to it.
         * 1. If this is a bulk prefetch, we first check if we are in bulk transition.
         *   a. If not, check if we can start bulk transition.
         *     i. If can't, we start eviction for blocked line.
         *     ii. If can, we start bulk transition to process the current line.
         *   b. If we are already in bulk transition, we just processing the current line.
         *      It is possible that we find no space for the current line, (two lines
         *      mapped to the same cache set, but we can only check for one line
         *      availability). In such case, we have to exit bulk transition and start
         *      eviction again.
         */
        Addr lineAddr := in_msg.LineAddress;
        bool isInBulk := false;
        if (!in_msg.addrBulk.empty()) {
          // This is a bulk prefetch request.
          // First check if we are already in BulkTransition.
          isInBulk := in_msg.addrBulk.isInBulkTransition(); 
          if (!isInBulk) {
            // We are not in bulk transistion yet. Check if we can start.
            CanStartBulkPrefetchRet ret := canStartBulkPrefetch(in_msg);
            if (ret.canStart) {
              // We can start bulk prefetch.
              in_msg.addrBulk.startBulkTransition();
              lineAddr := in_msg.addrBulk.getCurrentLineInBulk();
            } else if (ret.blockedByLine) {
              // We are blocked by this line. So start to process it.
              lineAddr := ret.blockLine;
            } else {
              // This is resource stall.
            }
          } else {
            // We are already in bulk transistion, get the current line.
            lineAddr := in_msg.addrBulk.getCurrentLineInBulk();
          }
        }
        Entry cache_entry := getCacheEntry(lineAddr);
        TBE tbe := TBEs[lineAddr];
        if (is_valid(cache_entry)) {
          // The block to be prefetched is already cached.
          trigger(prefetch_request_type_to_event(in_msg.Type),
                  lineAddr, cache_entry, tbe);
        }
        // Miss.
        if (cache.cacheAvail(lineAddr)) {
          // We have space for it.
          trigger(prefetch_request_type_to_event(in_msg.Type),
                  lineAddr, cache_entry, tbe);
        } else {
          // Find a victim.
          if (isInBulk) {
            /**
             * Somehow we don't have space for bulk transistion, even
             * though we guarantee that in canStartBulkPrefetch. This
             * is because if two lines are mapped to the same cache set,
             * the current API only allows us to check for one line
             * availability, and thus we wrongly thought that we have
             * enough space.
             * In such case, we quit bulk transistion and start normal
             * eviction so that we can restart bulk transistion later.
             */
             in_msg.addrBulk.exitBulkTransition();
          }
          Addr victimAddr := cache.cacheProbe(lineAddr);
          Entry victimEntry := getCacheEntry(victimAddr);
          TBE victimTBE := TBEs[victimAddr];
          if (is_valid(victimEntry) && inL0Cache(victimEntry.CacheState)) {
            trigger(Event:PF_L0_Invalidate_Own,
                    victimAddr, victimEntry, victimTBE);
          } else {
            trigger(Event:PF_L1_Replacement,
                    victimAddr, victimEntry, victimTBE);
          }
        }
      }
    }
  }

  void enqueuePrefetch(Addr address, RubyRequestType type) {
    enqueue(prefetchQueue_out, RubyRequest, 1) {
      out_msg.LineAddress := address;
      out_msg.Type := type;
      out_msg.AccessMode := RubyAccessMode:Supervisor;
    }
  }
  void enqueueBulkPrefetch(Addr address, RubyRequestType type,
                           RubyAddressBulk addrBulk) {
    enqueue(prefetchQueue_out, RubyRequest, 1) {
      out_msg.LineAddress := address;
      out_msg.Type := type;
      out_msg.AccessMode := RubyAccessMode:Supervisor;
      out_msg.addrBulk := addrBulk;
    }
  }

  // ACTIONS
  action(a_issueGETS, "a", desc="Issue GETS") {
    peek(messageBufferFromL0_in, CoherenceMsg) {
      enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
        out_msg.addr := address;
        out_msg.Type := CoherenceRequestType:GETS;
        out_msg.Requestors.add(machineID);
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                          l2_select_low_bit, l2_select_num_bits, clusterID));
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
        out_msg.AccessMode := in_msg.AccessMode;
        // Remember to copy the statistic hook.
        out_msg.requestStatistic := in_msg.requestStatistic;
      }
    }
  }

  action(au_issueGETU, "au", desc="Issue GETU") {
    peek(messageBufferFromL0_in, CoherenceMsg) {
      enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
        out_msg.addr := address;
        out_msg.Type := CoherenceRequestType:GETU;
        out_msg.Requestors.add(machineID);
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                          l2_select_low_bit, l2_select_num_bits, clusterID));
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
        out_msg.AccessMode := in_msg.AccessMode;
        // Remember to copy the statistic hook.
        out_msg.requestStatistic := in_msg.requestStatistic;
      }
    }
  }

  action(b_issueGETX, "b", desc="Issue GETX") {
    peek(messageBufferFromL0_in, CoherenceMsg) {
      enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
        out_msg.addr := address;
        out_msg.Type := CoherenceRequestType:GETX;
        out_msg.Requestors.add(machineID);
        DPRINTF(RubySlicc, "%s\n", machineID);
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                          l2_select_low_bit, l2_select_num_bits, clusterID));
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
        out_msg.AccessMode := in_msg.AccessMode;
        // Remember to copy the statistic hook.
        out_msg.requestStatistic := in_msg.requestStatistic;
      }
    }
  }

  action(c_issueUPGRADE, "c", desc="Issue GETX") {
    peek(messageBufferFromL0_in, CoherenceMsg) {
      enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
        out_msg.addr := address;
        out_msg.Type := CoherenceRequestType:UPGRADE;
        out_msg.Requestors.add(machineID);
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                          l2_select_low_bit, l2_select_num_bits, clusterID));
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
        out_msg.AccessMode := in_msg.AccessMode;
      }
    }
  }

  action(d_sendDataToRequestor, "d", desc="send data to requestor") {
    peek(requestNetwork_in, RequestMsg) {
      enqueue(responseNetwork_out, ResponseMsg, l1_response_latency) {
        assert(is_valid(cache_entry));
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.Dirty := cache_entry.Dirty;
        out_msg.Sender := machineID;
        out_msg.Destination.addNetDest(in_msg.Requestors);
        out_msg.MessageSize := MessageSizeType:Response_Data;
        out_msg.sliceIds := in_msg.sliceIds;
        // Try to detect subline.
        if (in_msg.sliceIds.isValid() && in_msg.Type == CoherenceRequestType:GETU && isStreamSublineEnabled()) {
          out_msg.MessageSize := getMessageSizeType(in_msg.sliceIds.firstSliceId().getSize());
        }
      }
    }
  }

  action(d2_sendDataToL2, "d2", desc="send data to the L2 cache because of M downgrade") {
    enqueue(responseNetwork_out, ResponseMsg, l1_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                          l2_select_low_bit, l2_select_num_bits, clusterID));
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(dt_sendDataToRequestor_fromTBE, "dt", desc="send data to requestor") {
    peek(requestNetwork_in, RequestMsg) {
      enqueue(responseNetwork_out, ResponseMsg, l1_response_latency) {
        assert(is_valid(tbe));
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.Dirty := tbe.Dirty;
        out_msg.Sender := machineID;
        out_msg.Destination.addNetDest(in_msg.Requestors);
        out_msg.MessageSize := MessageSizeType:Response_Data;
        out_msg.sliceIds := in_msg.sliceIds;
      }
    }
  }

  action(d2t_sendDataToL2_fromTBE, "d2t", desc="send data to the L2 cache") {
    enqueue(responseNetwork_out, ResponseMsg, l1_response_latency) {
      assert(is_valid(tbe));
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                          l2_select_low_bit, l2_select_num_bits, clusterID));
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(e_sendAckToRequestor, "e", desc="send invalidate ack to requestor (could be L2 or L1)") {
    peek(requestNetwork_in, RequestMsg) {
      enqueue(responseNetwork_out, ResponseMsg, l1_response_latency) {
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.addNetDest(in_msg.Requestors);
        out_msg.MessageSize := MessageSizeType:Response_Control;
      }
    }
  }

  action(f_sendDataToL2, "f", desc="send data to the L2 cache") {
    enqueue(responseNetwork_out, ResponseMsg, l1_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                          l2_select_low_bit, l2_select_num_bits, clusterID));
      out_msg.MessageSize := MessageSizeType:Writeback_Data;
    }
  }

  action(ft_sendDataToL2_fromTBE, "ft", desc="send data to the L2 cache") {
    enqueue(responseNetwork_out, ResponseMsg, l1_response_latency) {
      assert(is_valid(tbe));
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                          l2_select_low_bit, l2_select_num_bits, clusterID));
      out_msg.MessageSize := MessageSizeType:Writeback_Data;
    }
  }

  action(fi_sendInvAck, "fi", desc="send inv ack to the requestor") {
    peek(requestNetwork_in, RequestMsg) {
      enqueue(responseNetwork_out, ResponseMsg, l1_response_latency) {
        out_msg.addr := address;
        out_msg.Type := CoherenceResponseType:ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.addNetDest(in_msg.Requestors);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        out_msg.AckCount := 1;
      }
    }
  }

  action(forward_eviction_to_L0, "\cc", desc="sends eviction information to the processor") {
      enqueue(bufferToL0_out, CoherenceMsg, l1_response_latency) {
          out_msg.addr := address;
          out_msg.Class := CoherenceClass:INV;
          out_msg.Sender := machineID;
          out_msg.Dest := createMachineID(MachineType:L0Cache, version);
          out_msg.MessageSize := MessageSizeType:Control;
      }
  }

  action(g_issuePUTX, "g", desc="send data to the L2 cache") {
    enqueue(requestNetwork_out, RequestMsg, l1_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.addr := address;
      out_msg.Type := CoherenceRequestType:PUTX;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Requestors.add(machineID);
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                          l2_select_low_bit, l2_select_num_bits, clusterID));
      if (cache_entry.Dirty) {
        out_msg.MessageSize := MessageSizeType:Writeback_Data;
        out_msg.DataBlk := cache_entry.DataBlk;
      } else {
        out_msg.MessageSize := MessageSizeType:Writeback_Control;
        // This is considered a control evict message.
        addNoCControlMsgs(cache_entry.ReqStat, 1);
        addNoCControlEvictMsgs(cache_entry.ReqStat, 1);
      }
    }
  }

  action(j_sendUnblock, "j", desc="send unblock to the L2 cache") {
    enqueue(unblockNetwork_out, ResponseMsg, to_l2_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:UNBLOCK;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                          l2_select_low_bit, l2_select_num_bits, clusterID));
      out_msg.MessageSize := MessageSizeType:Response_Control;
      DPRINTF(RubySlicc, "%#x\n", address);
    }
  }

  action(jj_sendExclusiveUnblock, "\j", desc="send unblock to the L2 cache") {
    enqueue(unblockNetwork_out, ResponseMsg, to_l2_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:EXCLUSIVE_UNBLOCK;
      out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                          l2_select_low_bit, l2_select_num_bits, clusterID));
      out_msg.MessageSize := MessageSizeType:Response_Control;
      DPRINTF(RubySlicc, "%#x\n", address);

    }
  }

  action(h_data_to_l0, "h", desc="If not prefetch, send data to the L0 cache.") {
    enqueue(bufferToL0_out, CoherenceMsg, l1_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.addr := address;
      out_msg.Class := CoherenceClass:DATA;
      out_msg.Sender := machineID;
      out_msg.Dest := createMachineID(MachineType:L0Cache, version);
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(hh_xdata_to_l0, "\h", desc="If not prefetch, notify sequencer that store completed.") {
    enqueue(bufferToL0_out, CoherenceMsg, l1_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.addr := address;
      out_msg.Class := CoherenceClass:DATA_EXCLUSIVE;
      out_msg.Sender := machineID;
      out_msg.Dest := createMachineID(MachineType:L0Cache, version);
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(h_stale_data_to_l0, "hs", desc="If not prefetch, send data to the L0 cache.") {
    enqueue(bufferToL0_out, CoherenceMsg, l1_response_latency) {
      assert(is_valid(cache_entry));
      out_msg.addr := address;
      out_msg.Class := CoherenceClass:STALE_DATA;
      out_msg.Sender := machineID;
      out_msg.Dest := createMachineID(MachineType:L0Cache, version);
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.MessageSize := MessageSizeType:Response_Data;
     }
  }

  action(hu_xdata_to_l0, "hu", desc="Uncached exclusive data for L0") {
    enqueue(bufferToL0_out, CoherenceMsg, l1_response_latency) {
      assert(is_valid(tbe));
      out_msg.addr := address;
      out_msg.Class := CoherenceClass:DATA_EXCLUSIVE;
      out_msg.Sender := machineID;
      out_msg.Dest := createMachineID(MachineType:L0Cache, version);
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }


  action(i_allocateTBE, "i", desc="Allocate TBE (number of invalidates=0)") {
    check_allocate(TBEs);
    assert(is_valid(cache_entry));
    TBEs.allocate(address);
    set_tbe(TBEs[address]);
    tbe.Dirty := cache_entry.Dirty;
    tbe.DataBlk := cache_entry.DataBlk;
  }

  action(iu_allocateTBEUncache, "iu", desc="Allocate TBE for GETU") {
    check_allocate(TBEs);
    TBEs.allocate(address);
    set_tbe(TBEs[address]);
    tbe.Dirty := false;
  }

  action(tu_setTBEForGetU, "tu", desc="Set TBE entry for GetU request") {
    peek(messageBufferFromL0_in, CoherenceMsg) {
      assert(is_valid(tbe));
      assert(in_msg.Class == CoherenceClass:GETU);
      tbe.cachedUpper := false;
    }
  }

  action(trs_copyReqStatToTBE, "trs", desc="Copy ReqStat to TBE") {
    peek(messageBufferFromL0_in, CoherenceMsg) {
      assert(is_valid(tbe));
      tbe.ReqStat := in_msg.requestStatistic;
      // To simplify actions, this implicitly means one control message
      // to send the request to LLC, and one data message.
      addNoCControlMsgs(tbe.ReqStat, 1);
      addNoCDataMsgs(tbe.ReqStat, 1);
    }
  }

  action(crs_copyReqStatFromTBEToCacheEntry, "crs", desc="Copy ReqStat from TBE to cache entry.") {
    assert(is_valid(tbe));
    assert(is_valid(cache_entry));
    cache_entry.ReqStat := tbe.ReqStat;
  }

  action(k_popL0RequestQueue, "k", desc="Pop mandatory queue.") {
    messageBufferFromL0_in.dequeue(clockEdge());
  }

  action(l_popL2RequestQueue, "l",
         desc="Pop incoming request queue and profile the delay within this virtual network") {
    Tick delay := requestNetwork_in.dequeue(clockEdge());
    profileMsgDelay(2, ticksToCycles(delay));
  }

  action(o_popL2ResponseQueue, "o",
         desc="Pop Incoming Response queue and profile the delay within this virtual network") {
    Tick delay := responseNetwork_in.dequeue(clockEdge());
    profileMsgDelay(1, ticksToCycles(delay));
  }

  action(s_deallocateTBE, "s", desc="Deallocate TBE") {
    TBEs.deallocate(address);
    unset_tbe();
  }

  action(u_writeDataFromL0Request, "ureql0", desc="Write data to cache") {
    peek(messageBufferFromL0_in, CoherenceMsg) {
      assert(is_valid(cache_entry));
      if (in_msg.Dirty) {
          cache_entry.DataBlk := in_msg.DataBlk;
          cache_entry.Dirty := in_msg.Dirty;
      }
    }
  }

  action(u_writeDataFromL2Response, "uresl2", desc="Write data to cache") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk := in_msg.DataBlk;
    }
  }

  action(u_writeDataFromL2ResponseToTBE, "uresl2tbe", desc="Write data to tbe") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(is_valid(tbe));
      tbe.DataBlk := in_msg.DataBlk;
    }
  }

  action(u_writeDataFromL0Response, "uresl0", desc="Write data to cache") {
    peek(messageBufferFromL0_in, CoherenceMsg) {
      assert(is_valid(cache_entry));
      if (in_msg.Dirty) {
          cache_entry.DataBlk := in_msg.DataBlk;
          cache_entry.Dirty := in_msg.Dirty;
      }
    }
  }

  action(q_updateAckCount, "q", desc="Update ack count") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(is_valid(tbe));
      tbe.pendingAcks := tbe.pendingAcks - in_msg.AckCount;
      APPEND_TRANSITION_COMMENT(in_msg.AckCount);
      APPEND_TRANSITION_COMMENT(" p: ");
      APPEND_TRANSITION_COMMENT(tbe.pendingAcks);
    }
  }

  action(drsc_addNoCCtrlMsgToCacheEntry, "drsc", desc="Add one ctrl msg to cache ReqStat") {
    assert(is_valid(cache_entry));
    addNoCControlMsgs(cache_entry.ReqStat, 1);
  }
  action(drse_addNoCCtrlEvictMsgToCacheEntry, "drse", desc="Add one ctrl evict msg to cache ReqStat") {
    assert(is_valid(cache_entry));
    addNoCControlEvictMsgs(cache_entry.ReqStat, 1);
  }

  action(ff_deallocateCacheBlock, "\f",
         desc="Deallocate L1 cache block.") {
    if (cache.isTagPresent(address)) {
      cache.deallocate(address);
    }
    if (is_valid(cache_entry)) {
      ++cache.deallocated;
      // If this is dirty then we considered it has been reused.
      bool reused := cache_entry.Reused || cache_entry.Dirty;
      if (!reused) {
        ++cache.deallocated_no_reuse;

        if (isRequestStatisticValid(cache_entry.ReqStat)) {
          // Record msg stats.
          recordDeallocateNoReuseReqStats(cache_entry.ReqStat, cache);
        } else {
          ++cache.deallocated_no_reuse_no_req_stat;
        }
      }
    }
    unset_cache_entry();
  }

  action(oo_allocateCacheBlock, "\o", desc="Set cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      set_cache_entry(cache.allocate(address, new Entry));
    }
  }

  action(z0_stallAndWaitL0Queue, "\z0", desc="recycle L0 request queue") {
    stall_and_wait(messageBufferFromL0_in, address);
  }

  action(zp_stallAndWaitPrefetchQueue, "\zp", desc="recycle prefetch queue") {
    stall_and_wait(prefetchQueue_in, address);
  }

  action(z2_stallAndWaitL2Queue, "\z2", desc="recycle L2 request queue") {
    stall_and_wait(requestNetwork_in, address);
  }

  action(kd_wakeUpDependents, "kd", desc="wake-up dependents") {
    wakeUpAllBuffers(address);
  }

  action(uu_profileMiss, "\um", desc="Profile the demand miss") {
    ++cache.demand_misses;
  }

  action(uu_profileHit, "\uh", desc="Profile the demand hit") {
    ++cache.demand_hits;
    if (is_valid(cache_entry)) {
      cache_entry.Reused := true;
    }
  }

  /**
   * Actions for prefetcher.
   */
  action(ph_observeHit, "\ph", desc="Inform the prefetcher about the hit") {
    if (enable_prefetch && cache_entry.isPrefetch) {
      prefetcher.observePfHit(address);
      cache_entry.isPrefetch := false;
    }
  }
  action(pm_observeMiss, "\pm", desc="Inform the prefetcher about the miss") {
    if (enable_prefetch) {
      // We always use LD as the prefetch type.
      prefetcher.observeMiss(address, RubyRequestType:LD);
    }
  }
  action(ppm_observePfMiss, "\ppm",
         desc="Inform the prefetcher about the partial miss") {
    if (enable_prefetch) {
      prefetcher.observePfMiss(address);
    }
  }
  action(mp_markPrefetched, "mp", desc="Set the isPrefetch flag") {
    assert(is_valid(cache_entry));
    cache_entry.isPrefetch := true;
  }
  action(ap_issuePfGETS, "ap", desc="Issue prefetch GETS") {
    peek(prefetchQueue_in, RubyRequest) {
      bool shouldEnqueue := true;
      enqueue(requestNetwork_out, RequestMsg, l1_request_latency, shouldEnqueue) {
        out_msg.addr := address;
        out_msg.Type := CoherenceRequestType:GETS;
        out_msg.Requestors.add(machineID);
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                          l2_select_low_bit, l2_select_num_bits, clusterID));
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
        out_msg.Prefetch := in_msg.Prefetch;
        out_msg.AccessMode := in_msg.AccessMode;
        // Now we decide if we have to do bulk prefetch.
        bool isBulk := !in_msg.addrBulk.empty();
        if (isBulk) {
          assert(in_msg.addrBulk.isInBulkTransition());
          if (in_msg.addrBulk.mergeToBulkMessage(out_msg_ptr)) {
            // This message is chained to previous bulk message.
            shouldEnqueue := false;
          }
        }
      }
    }
  }
  action(kp_popPrefetchQueue, "kp", desc="Pop prefetch queue.") {
    peek(prefetchQueue_in, RubyRequest) {
      bool isBulk := !in_msg.addrBulk.empty();
      if (isBulk) {
        // Bulk prefetch request if this is the last line
        assert(in_msg.addrBulk.isInBulkTransition());
        if (in_msg.addrBulk.isLastLineInBulk()) {
          prefetchQueue_in.dequeue(clockEdge());
        } else {
          in_msg.addrBulk.stepLineInBulk();
        }
      } else {
        // Normal prefetch request.
        prefetchQueue_in.dequeue(clockEdge());
      }
    }
  }


  /**
   * ! Sean: StreamAwareCache
   */
  action(nhs_notifyHitStreamLoadV, "nhs", desc="Notify the SE a HitStreamLoadV") {
    peek(messageBufferFromL0_in, CoherenceMsg) {
      if (se.isStreamOffloaded(in_msg.slice)) {
        se.receiveOffloadStreamRequestHit(in_msg.slice);
      }
    }
  }

  action(mt_multicastTestReceived, "mt", desc="MulticastTestReceived") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(multicast_test);
      DPRINTF(RubyMulticast, "Multicast received from %s.\n", in_msg.Sender);
    }
  }


  //*****************************************************
  // TRANSITIONS
  //*****************************************************

  // Transitions for Load/Store/Replacement/WriteBack from transient states
  transition({IS, IU, IM, IS_I, M_I, SM, SINK_WB_ACK, S_IL0, M_IL0, E_IL0, MM_IL0},
             {Load, LoadV, HitStreamLoadV, LoadU, Store, L1_Replacement}) {
    z0_stallAndWaitL0Queue;
  }
  transition({PF_IS,PF_IS_I}, {Store, L1_Replacement}) {
    z0_stallAndWaitL0Queue;
  }
  transition({IS,IM,IS_I,M_I,SM,SINK_WB_ACK,S_IL0,M_IL0,E_IL0,MM_IL0,PF_IS,PF_IS_I},
             PF_L1_Replacement) {
    zp_stallAndWaitPrefetchQueue;
  }

  transition(I, Load, IS) {
    oo_allocateCacheBlock;
    i_allocateTBE;
    trs_copyReqStatToTBE;
    a_issueGETS;
    uu_profileMiss;
    pm_observeMiss;
    k_popL0RequestQueue;
  }

  transition(I, PF_Load, PF_IS) {
    oo_allocateCacheBlock;
    i_allocateTBE;
    ap_issuePfGETS;
    kp_popPrefetchQueue;
  }
  transition(PF_IS, Load, IS) {
    trs_copyReqStatToTBE;
    uu_profileMiss,
    ppm_observePfMiss;
    k_popL0RequestQueue;
  }
  transition(PF_IS_I, Load, IS_I) {
    trs_copyReqStatToTBE;
    uu_profileMiss;
    ppm_observePfMiss;
    k_popL0RequestQueue;
  }

  transition(I, LoadV, IS) {
    oo_allocateCacheBlock;
    i_allocateTBE;
    tu_setTBEForGetU;
    trs_copyReqStatToTBE;
    a_issueGETS;
    uu_profileMiss;
    pm_observeMiss;
    k_popL0RequestQueue;
  }

  transition(I, LoadU, IU) {
    iu_allocateTBEUncache;
    tu_setTBEForGetU;
    trs_copyReqStatToTBE;
    au_issueGETU;
    uu_profileMiss;
    pm_observeMiss;
    k_popL0RequestQueue;
  }

  transition(I, Store, IM) {
    oo_allocateCacheBlock;
    i_allocateTBE;
    trs_copyReqStatToTBE;
    b_issueGETX;
    uu_profileMiss;
    pm_observeMiss;
    k_popL0RequestQueue;
  }

  transition(I, Inv) {
    fi_sendInvAck;
    l_popL2RequestQueue;
  }

  transition(I, Data_Exclusive, I) {
    mt_multicastTestReceived;
    o_popL2ResponseQueue;
  }

  // Transitions from Shared
  transition({S,SS}, Load, S) {
    h_data_to_l0;
    uu_profileHit;
    ph_observeHit;
    k_popL0RequestQueue;
  }

  transition(EE, Load, E) {
    hh_xdata_to_l0;
    uu_profileHit;
    ph_observeHit;
    k_popL0RequestQueue;
  }

  transition(EE, LoadV, EE) {
    hh_xdata_to_l0;
    uu_profileHit;
    ph_observeHit;
    k_popL0RequestQueue;
  }
  transition(SS, LoadV, SS) {
    hh_xdata_to_l0;
    uu_profileHit;
    ph_observeHit;
    k_popL0RequestQueue;
  }
  transition(S, LoadV, S) {
    hh_xdata_to_l0;
    uu_profileHit;
    ph_observeHit;
    k_popL0RequestQueue;
  }
  transition(MM, LoadV, MM) {
    hh_xdata_to_l0;
    uu_profileHit;
    ph_observeHit;
    k_popL0RequestQueue;
  }

  transition(EE, HitStreamLoadV, EE) {
    hh_xdata_to_l0;
    uu_profileHit;
    ph_observeHit;
    nhs_notifyHitStreamLoadV;
    k_popL0RequestQueue;
  }
  transition(SS, HitStreamLoadV, SS) {
    hh_xdata_to_l0;
    uu_profileHit;
    ph_observeHit;
    nhs_notifyHitStreamLoadV;
    k_popL0RequestQueue;
  }
  transition(S, HitStreamLoadV, S) {
    hh_xdata_to_l0;
    uu_profileHit;
    ph_observeHit;
    nhs_notifyHitStreamLoadV;
    k_popL0RequestQueue;
  }
  transition(MM, HitStreamLoadV, MM) {
    hh_xdata_to_l0;
    uu_profileHit;
    ph_observeHit;
    nhs_notifyHitStreamLoadV;
    k_popL0RequestQueue;
  }

  transition(MM, Load, M) {
    hh_xdata_to_l0;
    uu_profileHit;
    ph_observeHit;
    k_popL0RequestQueue;
  }

  transition({S,SS}, Store, SM) {
    i_allocateTBE;
    trs_copyReqStatToTBE;
    c_issueUPGRADE;
    uu_profileMiss;
    k_popL0RequestQueue;
  }

  transition(SS, {L1_Replacement, PF_L1_Replacement}, I) {
    ff_deallocateCacheBlock;
  }

  transition(S, {L0_Invalidate_Own, PF_L0_Invalidate_Own, L0_Invalidate_Else}, S_IL0) {
    forward_eviction_to_L0;
  }

  transition(SS, Inv, I) {
    fi_sendInvAck;
    // We consider Inv and InvAck as 2 msgs.
    drsc_addNoCCtrlMsgToCacheEntry;
    drsc_addNoCCtrlMsgToCacheEntry;
    drse_addNoCCtrlEvictMsgToCacheEntry;
    drse_addNoCCtrlEvictMsgToCacheEntry;
    ff_deallocateCacheBlock;
    l_popL2RequestQueue;
  }

  // Transitions from Exclusive

  transition({EE,MM}, Store, M) {
    hh_xdata_to_l0;
    uu_profileHit;
    ph_observeHit;
    k_popL0RequestQueue;
  }

  transition(EE, {L1_Replacement, PF_L1_Replacement}, M_I) {
    // silent E replacement??
    i_allocateTBE;
    g_issuePUTX;   // send data, but hold in case forwarded request
    ff_deallocateCacheBlock;
  }

  transition(EE, Inv, I) {
    // don't send data
    fi_sendInvAck;
    // We considier Inv and InvAck as 2 msgs.
    drsc_addNoCCtrlMsgToCacheEntry;
    drsc_addNoCCtrlMsgToCacheEntry;
    drse_addNoCCtrlEvictMsgToCacheEntry;
    drse_addNoCCtrlEvictMsgToCacheEntry;
    ff_deallocateCacheBlock;
    l_popL2RequestQueue;
  }

  transition(EE, Fwd_GETX, I) {
    d_sendDataToRequestor;
    ff_deallocateCacheBlock;
    l_popL2RequestQueue;
  }

  transition(EE, Fwd_GETS, SS) {
    d_sendDataToRequestor;
    d2_sendDataToL2;
    l_popL2RequestQueue;
  }

  transition(E,
    {L0_Invalidate_Own, PF_L0_Invalidate_Own, L0_Invalidate_Else}, E_IL0) {
    forward_eviction_to_L0;
  }

  // Transitions from Modified
  transition(MM, {L1_Replacement, PF_L1_Replacement}, M_I) {
    i_allocateTBE;
    g_issuePUTX;   // send data, but hold in case forwarded request
    ff_deallocateCacheBlock;
  }

  transition({M,E}, WriteBack, MM) {
    u_writeDataFromL0Request;
    k_popL0RequestQueue;
  }

  transition(M_I, WB_Ack, I) {
    s_deallocateTBE;
    o_popL2ResponseQueue;
    ff_deallocateCacheBlock;
    kd_wakeUpDependents;
  }

  transition(MM, Inv, I) {
    f_sendDataToL2;
    ff_deallocateCacheBlock;
    l_popL2RequestQueue;
  }

  transition(M_I, Inv, SINK_WB_ACK) {
    ft_sendDataToL2_fromTBE;
    l_popL2RequestQueue;
  }

  transition(MM, Fwd_GETX, I) {
    d_sendDataToRequestor;
    ff_deallocateCacheBlock;
    l_popL2RequestQueue;
  }

  transition(MM, Fwd_GETS, SS) {
    d_sendDataToRequestor;
    d2_sendDataToL2;
    l_popL2RequestQueue;
  }

  transition(M,
    {L0_Invalidate_Own, PF_L0_Invalidate_Own, L0_Invalidate_Else}, M_IL0) {
    forward_eviction_to_L0;
  }

  transition(M_I, Fwd_GETX, SINK_WB_ACK) {
    dt_sendDataToRequestor_fromTBE;
    l_popL2RequestQueue;
  }

  transition(M_I, Fwd_GETS, SINK_WB_ACK) {
    dt_sendDataToRequestor_fromTBE;
    d2t_sendDataToL2_fromTBE;
    l_popL2RequestQueue;
  }

  // Transitions from IS
  transition({IS,IS_I}, Inv, IS_I) {
    fi_sendInvAck;
    l_popL2RequestQueue;
  }
  
  transition({PF_IS, PF_IS_I}, Inv, PF_IS_I) {
    fi_sendInvAck;
    l_popL2RequestQueue;
  }

  transition(IS, Data_all_Acks, S) {
    u_writeDataFromL2Response;
    h_data_to_l0;
    crs_copyReqStatFromTBEToCacheEntry;
    s_deallocateTBE;
    o_popL2ResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_IS, Data_all_Acks, SS) {
    u_writeDataFromL2Response;
    s_deallocateTBE;
    mp_markPrefetched;
    o_popL2ResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS_I, Data_all_Acks, I) {
    u_writeDataFromL2Response;
    h_stale_data_to_l0;
    crs_copyReqStatFromTBEToCacheEntry;
    s_deallocateTBE;
    ff_deallocateCacheBlock;
    o_popL2ResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_IS_I, Data_all_Acks, I) {
    s_deallocateTBE;
    ff_deallocateCacheBlock;
    o_popL2ResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS, DataS_fromL1, S) {
    u_writeDataFromL2Response;
    j_sendUnblock;
    h_data_to_l0;
    crs_copyReqStatFromTBEToCacheEntry;
    // Unblock is also a control message.
    drsc_addNoCCtrlMsgToCacheEntry;
    s_deallocateTBE;
    o_popL2ResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_IS, DataS_fromL1, SS) {
    u_writeDataFromL2Response;
    mp_markPrefetched;
    j_sendUnblock;
    // Unblock is also a control message.
    drsc_addNoCCtrlMsgToCacheEntry;
    s_deallocateTBE;
    o_popL2ResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS_I, DataS_fromL1, I) {
    u_writeDataFromL2Response;
    j_sendUnblock;
    h_stale_data_to_l0;
    crs_copyReqStatFromTBEToCacheEntry;
    // Unblock is also a control message.
    drsc_addNoCCtrlMsgToCacheEntry;
    s_deallocateTBE;
    ff_deallocateCacheBlock;
    o_popL2ResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_IS_I, DataS_fromL1, I) {
    j_sendUnblock;
    // Unblock is also a control message.
    drsc_addNoCCtrlMsgToCacheEntry;
    s_deallocateTBE;
    ff_deallocateCacheBlock;
    o_popL2ResponseQueue;
    kd_wakeUpDependents;
  }

  // directory is blocked when sending exclusive data
  transition({IS,IS_I}, Data_Exclusive, E) {
    u_writeDataFromL2Response;
    hh_xdata_to_l0;
    jj_sendExclusiveUnblock;
    crs_copyReqStatFromTBEToCacheEntry;
    // Unblock is also a control message.
    drsc_addNoCCtrlMsgToCacheEntry;
    s_deallocateTBE;
    o_popL2ResponseQueue;
    kd_wakeUpDependents;
  }

  // directory is blocked when sending exclusive data
  transition({PF_IS,PF_IS_I}, Data_Exclusive, EE) {
    u_writeDataFromL2Response;
    mp_markPrefetched;
    jj_sendExclusiveUnblock;
    // Unblock is also a control message.
    drsc_addNoCCtrlMsgToCacheEntry;
    s_deallocateTBE;
    o_popL2ResponseQueue;
    kd_wakeUpDependents;
  }

  // The data will not be cached at upper level.
  transition({IS, IS_I}, Data_ExclusiveV, EE) {
    u_writeDataFromL2Response;
    hh_xdata_to_l0;
    jj_sendExclusiveUnblock;
    crs_copyReqStatFromTBEToCacheEntry;
    // Unblock is also a control message.
    drsc_addNoCCtrlMsgToCacheEntry;
    s_deallocateTBE;
    o_popL2ResponseQueue;
    kd_wakeUpDependents;
  }

  // Lower level is not expecting an ExclusiveUnblock.
  transition(IU, Data_ExclusiveV, I) {
    u_writeDataFromL2ResponseToTBE;
    hu_xdata_to_l0;
    s_deallocateTBE;
    o_popL2ResponseQueue;
    kd_wakeUpDependents;
  }

  // Transitions from IM
  transition({IM,SM}, Inv, IM) {
    fi_sendInvAck;
    l_popL2RequestQueue;
  }

  transition(IM, Data, SM) {
    u_writeDataFromL2Response;
    q_updateAckCount;
    o_popL2ResponseQueue;
  }

  transition(IM, Data_all_Acks, M) {
    u_writeDataFromL2Response;
    hh_xdata_to_l0;
    jj_sendExclusiveUnblock;
    crs_copyReqStatFromTBEToCacheEntry;
    // Unblock is also a control message.
    drsc_addNoCCtrlMsgToCacheEntry;
    s_deallocateTBE;
    o_popL2ResponseQueue;
    kd_wakeUpDependents;
  }

  transition({SM, IM}, Ack) {
    q_updateAckCount;
    o_popL2ResponseQueue;
  }

  transition(SM, Ack_all, M) {
    jj_sendExclusiveUnblock;
    hh_xdata_to_l0;
    s_deallocateTBE;
    o_popL2ResponseQueue;
    kd_wakeUpDependents;
  }

  transition(SM, L0_Invalidate_Else, SM_IL0) {
    forward_eviction_to_L0;
  }

  transition(SINK_WB_ACK, Inv){
    fi_sendInvAck;
    l_popL2RequestQueue;
  }

  transition(SINK_WB_ACK, WB_Ack, I){
    s_deallocateTBE;
    o_popL2ResponseQueue;
    ff_deallocateCacheBlock;
    kd_wakeUpDependents;
  }

  transition({M_IL0, E_IL0}, WriteBack, MM_IL0) {
    u_writeDataFromL0Request;
    k_popL0RequestQueue;
    kd_wakeUpDependents;
  }

  transition({M_IL0, E_IL0}, L0_DataAck, MM) {
    u_writeDataFromL0Response;
    k_popL0RequestQueue;
    kd_wakeUpDependents;
  }

  transition({M_IL0, MM_IL0}, L0_Ack, MM) {
    k_popL0RequestQueue;
    kd_wakeUpDependents;
  }

  transition(E_IL0, L0_Ack, EE) {
    k_popL0RequestQueue;
    kd_wakeUpDependents;
  }

  transition(S_IL0, L0_Ack, SS) {
    k_popL0RequestQueue;
    kd_wakeUpDependents;
  }

  transition(SM_IL0, L0_Ack, IM) {
    k_popL0RequestQueue;
    kd_wakeUpDependents;
  }

  transition({S_IL0, M_IL0, E_IL0, SM_IL0, SM}, L0_Invalidate_Own) {
    z0_stallAndWaitL0Queue;
  }
  transition({S_IL0, M_IL0, E_IL0, SM_IL0, SM}, PF_L0_Invalidate_Own) {
    zp_stallAndWaitPrefetchQueue;
  }

  transition({S_IL0, M_IL0, E_IL0, SM_IL0}, L0_Invalidate_Else) {
    z2_stallAndWaitL2Queue;
  }

  transition({S_IL0, M_IL0, E_IL0, MM_IL0}, {Inv, Fwd_GETX, Fwd_GETS}) {
    z2_stallAndWaitL2Queue;
  }

  transition({S,SS,E,EE,M,MM,IS,PF_IS,IU,IM,SM,IS_I,PF_IS_I,M_I,SINK_WB_ACK,S_IL0,E_IL0,M_IL0,MM_IL0,SM_IL0},
             {PF_Load}) {
    kp_popPrefetchQueue;
  }

  transition({I, S, SS, E, EE, M, MM, IS, IU, IM, SM, IS_I, M_I, SINK_WB_ACK, S_IL0, E_IL0, M_IL0, MM_IL0, SM_IL0},
             {L0_StreamConfig, L0_StreamEnd, L0_FwdStreamReq, L0_StreamNDC}) {
    k_popL0RequestQueue;
  }

  transition({I, S, SS, E, EE, M, MM, IS, IU, IM, SM, IS_I, M_I, SINK_WB_ACK, S_IL0, E_IL0, M_IL0, MM_IL0, SM_IL0}, L2_StreamData) {
    o_popL2ResponseQueue;
  }

  transition({I, S, SS, E, EE, M, MM, IS, IU, IM, SM, IS_I, M_I, SINK_WB_ACK, S_IL0, E_IL0, M_IL0, MM_IL0, SM_IL0}, L1_StreamData) {
    j_sendUnblock;
    o_popL2ResponseQueue;
  }

  transition({I, M_I, SINK_WB_ACK}, OffloadStreamLoad) {
    k_popL0RequestQueue; 
  }
}
